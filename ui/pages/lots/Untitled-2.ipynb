{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd024eb",
   "metadata": {},
   "source": [
    "# Tesseract OCR Demo\n",
    "\n",
    "This notebook demonstrates how to use Tesseract OCR for text extraction from images. We'll cover installation, basic usage, configuration options, and preprocessing techniques to improve OCR accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e21bbb",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "Install pytesseract and import required libraries including pytesseract, PIL/Pillow, and cv2 for image handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Tesseract OCR engine must be installed on your system\n",
    "# Ubuntu: sudo apt-get install tesseract-ocr\n",
    "# macOS: brew install tesseract\n",
    "\n",
    "# !pip install pytesseract pillow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify Tesseract is installed\n",
    "try:\n",
    "    tesseract_version = pytesseract.get_tesseract_version()\n",
    "    print(f\"Tesseract version: {tesseract_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Tesseract not found. Please install it first: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d30070",
   "metadata": {},
   "source": [
    "## 2. Load and Display an Image\n",
    "\n",
    "Load an image file using PIL or OpenCV and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image with text for demonstration\n",
    "# In practice, you would load an existing image file\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Create a sample image with text\n",
    "img_width, img_height = 600, 200\n",
    "sample_image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "draw = ImageDraw.Draw(sample_image)\n",
    "\n",
    "# Add sample text\n",
    "sample_text = \"Hello, Tesseract OCR!\\nThis is a demo image.\"\n",
    "try:\n",
    "    # Try to use a system font\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 32)\n",
    "except OSError:\n",
    "    # Fall back to default font\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "draw.text((50, 50), sample_text, fill='black', font=font)\n",
    "\n",
    "# Save the sample image\n",
    "sample_image_path = \"/tmp/sample_ocr_image.png\"\n",
    "sample_image.save(sample_image_path)\n",
    "print(f\"Sample image saved to: {sample_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b483fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the image using PIL\n",
    "pil_image = Image.open(sample_image_path)\n",
    "print(f\"Image size: {pil_image.size}\")\n",
    "print(f\"Image mode: {pil_image.mode}\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(pil_image)\n",
    "plt.title(\"Sample Image for OCR\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862de677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load using OpenCV\n",
    "cv_image = cv2.imread(sample_image_path)\n",
    "cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"OpenCV image shape: {cv_image.shape}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv_image_rgb)\n",
    "plt.title(\"Image loaded with OpenCV\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a036f",
   "metadata": {},
   "source": [
    "## 3. Basic Text Extraction with Tesseract\n",
    "\n",
    "Use `pytesseract.image_to_string()` to extract text from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text extraction from PIL Image\n",
    "extracted_text = pytesseract.image_to_string(pil_image)\n",
    "print(\"Extracted text from PIL Image:\")\n",
    "print(\"-\" * 40)\n",
    "print(extracted_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction from OpenCV image (numpy array)\n",
    "extracted_text_cv = pytesseract.image_to_string(cv_image_rgb)\n",
    "print(\"Extracted text from OpenCV Image:\")\n",
    "print(\"-\" * 40)\n",
    "print(extracted_text_cv)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text directly from file path\n",
    "extracted_text_path = pytesseract.image_to_string(sample_image_path)\n",
    "print(\"Extracted text from file path:\")\n",
    "print(\"-\" * 40)\n",
    "print(extracted_text_path)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eceea96",
   "metadata": {},
   "source": [
    "## 4. Configure Tesseract Options\n",
    "\n",
    "Set custom configuration options like PSM (Page Segmentation Mode) and OEM (OCR Engine Mode) for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740955d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page Segmentation Modes (PSM)\n",
    "# 0  = Orientation and script detection (OSD) only.\n",
    "# 1  = Automatic page segmentation with OSD.\n",
    "# 2  = Automatic page segmentation, but no OSD, or OCR.\n",
    "# 3  = Fully automatic page segmentation, but no OSD. (Default)\n",
    "# 4  = Assume a single column of text of variable sizes.\n",
    "# 5  = Assume a single uniform block of vertically aligned text.\n",
    "# 6  = Assume a single uniform block of text.\n",
    "# 7  = Treat the image as a single text line.\n",
    "# 8  = Treat the image as a single word.\n",
    "# 9  = Treat the image as a single word in a circle.\n",
    "# 10 = Treat the image as a single character.\n",
    "# 11 = Sparse text. Find as much text as possible in no particular order.\n",
    "# 12 = Sparse text with OSD.\n",
    "# 13 = Raw line. Treat the image as a single text line, bypassing Tesseract-specific hacks.\n",
    "\n",
    "print(\"Available PSM modes and their descriptions:\")\n",
    "psm_modes = {\n",
    "    0: \"OSD only\",\n",
    "    1: \"Automatic page segmentation with OSD\",\n",
    "    3: \"Fully automatic page segmentation (Default)\",\n",
    "    4: \"Single column of variable text sizes\",\n",
    "    6: \"Single uniform block of text\",\n",
    "    7: \"Single text line\",\n",
    "    8: \"Single word\",\n",
    "    10: \"Single character\",\n",
    "    11: \"Sparse text\",\n",
    "}\n",
    "for mode, desc in psm_modes.items():\n",
    "    print(f\"  PSM {mode}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different PSM modes\n",
    "# PSM 6: Assume a single uniform block of text\n",
    "config_psm6 = '--psm 6'\n",
    "text_psm6 = pytesseract.image_to_string(pil_image, config=config_psm6)\n",
    "print(\"PSM 6 (Single block):\")\n",
    "print(text_psm6)\n",
    "\n",
    "# PSM 7: Treat as a single text line\n",
    "config_psm7 = '--psm 7'\n",
    "text_psm7 = pytesseract.image_to_string(pil_image, config=config_psm7)\n",
    "print(\"PSM 7 (Single line):\")\n",
    "print(text_psm7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6171ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Engine Modes (OEM)\n",
    "# 0 = Legacy engine only.\n",
    "# 1 = Neural nets LSTM engine only.\n",
    "# 2 = Legacy + LSTM engines.\n",
    "# 3 = Default, based on what is available.\n",
    "\n",
    "print(\"OCR Engine Modes:\")\n",
    "print(\"  OEM 0: Legacy engine only\")\n",
    "print(\"  OEM 1: LSTM neural network engine only\")\n",
    "print(\"  OEM 2: Legacy + LSTM engines\")\n",
    "print(\"  OEM 3: Default (based on availability)\")\n",
    "\n",
    "# Using OEM 1 (LSTM) with PSM 6\n",
    "config_oem = '--oem 1 --psm 6'\n",
    "text_oem = pytesseract.image_to_string(pil_image, config=config_oem)\n",
    "print(f\"\\nWith OEM 1 + PSM 6:\")\n",
    "print(text_oem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578728ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional configuration options\n",
    "# Whitelist specific characters\n",
    "config_digits = '--psm 6 -c tessedit_char_whitelist=0123456789'\n",
    "print(\"Digits only whitelist config:\", config_digits)\n",
    "\n",
    "# Blacklist specific characters\n",
    "config_no_special = '--psm 6 -c tessedit_char_blacklist=@#$%^&*()'\n",
    "print(\"Blacklist special chars config:\", config_no_special)\n",
    "\n",
    "# Example with standard text\n",
    "text_default = pytesseract.image_to_string(pil_image, config='--psm 6')\n",
    "print(\"\\nDefault extraction:\")\n",
    "print(text_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a366d1",
   "metadata": {},
   "source": [
    "## 5. Extract Text from Specific Image Regions\n",
    "\n",
    "Crop specific regions of an image and extract text from those areas only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f805c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image with multiple text regions\n",
    "multi_region_img = Image.new('RGB', (800, 400), color='white')\n",
    "draw = ImageDraw.Draw(multi_region_img)\n",
    "\n",
    "try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "# Add text in different regions\n",
    "draw.text((50, 50), \"Region 1: Header Text\", fill='black', font=font)\n",
    "draw.text((50, 150), \"Region 2: Main Content\", fill='blue', font=font)\n",
    "draw.text((50, 250), \"Region 3: Footer Info\", fill='red', font=font)\n",
    "\n",
    "# Draw rectangles around regions for visualization\n",
    "draw.rectangle([40, 40, 350, 90], outline='green', width=2)\n",
    "draw.rectangle([40, 140, 380, 190], outline='green', width=2)\n",
    "draw.rectangle([40, 240, 350, 290], outline='green', width=2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(multi_region_img)\n",
    "plt.title(\"Image with Multiple Text Regions\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e857e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions of interest (ROI) as (left, upper, right, lower)\n",
    "regions = {\n",
    "    \"header\": (40, 40, 350, 90),\n",
    "    \"content\": (40, 140, 380, 190),\n",
    "    \"footer\": (40, 240, 350, 290),\n",
    "}\n",
    "\n",
    "# Extract text from each region\n",
    "print(\"Text extracted from specific regions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for region_name, box in regions.items():\n",
    "    # Crop the region\n",
    "    cropped = multi_region_img.crop(box)\n",
    "    \n",
    "    # Extract text from cropped region\n",
    "    text = pytesseract.image_to_string(cropped, config='--psm 7').strip()\n",
    "    \n",
    "    print(f\"\\n{region_name.upper()}:\")\n",
    "    print(f\"  Coordinates: {box}\")\n",
    "    print(f\"  Text: '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenCV for region extraction\n",
    "cv_multi = np.array(multi_region_img)\n",
    "\n",
    "# Define ROI using OpenCV format [y1:y2, x1:x2]\n",
    "roi_header = cv_multi[40:90, 40:350]\n",
    "roi_content = cv_multi[140:190, 40:380]\n",
    "\n",
    "# Display cropped regions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "axes[0].imshow(roi_header)\n",
    "axes[0].set_title(\"Header Region\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(roi_content)\n",
    "axes[1].set_title(\"Content Region\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract text from OpenCV crops\n",
    "print(\"Header:\", pytesseract.image_to_string(roi_header, config='--psm 7').strip())\n",
    "print(\"Content:\", pytesseract.image_to_string(roi_content, config='--psm 7').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00a42a",
   "metadata": {},
   "source": [
    "## 6. Get Bounding Boxes for Detected Text\n",
    "\n",
    "Use `image_to_boxes()` and `image_to_data()` to get character and word-level bounding boxes with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get character-level bounding boxes\n",
    "boxes = pytesseract.image_to_boxes(pil_image)\n",
    "print(\"Character-level bounding boxes (first 500 chars):\")\n",
    "print(boxes[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize character bounding boxes\n",
    "img_with_boxes = np.array(pil_image.copy())\n",
    "h, w = img_with_boxes.shape[:2]\n",
    "\n",
    "for box in boxes.splitlines():\n",
    "    parts = box.split()\n",
    "    if len(parts) >= 5:\n",
    "        char, x1, y1, x2, y2 = parts[0], int(parts[1]), int(parts[2]), int(parts[3]), int(parts[4])\n",
    "        # Tesseract uses bottom-left origin, convert to top-left\n",
    "        cv2.rectangle(img_with_boxes, (x1, h - y2), (x2, h - y1), (0, 255, 0), 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(img_with_boxes)\n",
    "plt.title(\"Character Bounding Boxes\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word-level data with confidence scores using image_to_data()\n",
    "import pandas as pd\n",
    "\n",
    "data = pytesseract.image_to_data(pil_image, output_type=pytesseract.Output.DICT)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter to only rows with text\n",
    "df_text = df[df['text'].str.strip() != ''].copy()\n",
    "\n",
    "print(\"Word-level OCR data:\")\n",
    "print(df_text[['level', 'left', 'top', 'width', 'height', 'conf', 'text']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word bounding boxes with confidence scores\n",
    "img_words = np.array(pil_image.copy())\n",
    "\n",
    "for _, row in df_text.iterrows():\n",
    "    x, y, w, h = row['left'], row['top'], row['width'], row['height']\n",
    "    conf = row['conf']\n",
    "    text = row['text']\n",
    "    \n",
    "    # Color based on confidence (green = high, red = low)\n",
    "    if conf > 80:\n",
    "        color = (0, 255, 0)  # Green\n",
    "    elif conf > 50:\n",
    "        color = (255, 165, 0)  # Orange\n",
    "    else:\n",
    "        color = (255, 0, 0)  # Red\n",
    "    \n",
    "    cv2.rectangle(img_words, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(img_words)\n",
    "plt.title(\"Word Bounding Boxes (Green=High Conf, Orange=Medium, Red=Low)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed013aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed statistics on confidence scores\n",
    "print(\"\\nConfidence Score Statistics:\")\n",
    "print(f\"  Mean: {df_text['conf'].mean():.2f}\")\n",
    "print(f\"  Min: {df_text['conf'].min()}\")\n",
    "print(f\"  Max: {df_text['conf'].max()}\")\n",
    "print(f\"  Words with conf > 80: {len(df_text[df_text['conf'] > 80])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b9aa1",
   "metadata": {},
   "source": [
    "## 7. Handle Different Languages\n",
    "\n",
    "Configure Tesseract to recognize text in different languages using the `lang` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available languages\n",
    "try:\n",
    "    langs = pytesseract.get_languages()\n",
    "    print(\"Available Tesseract languages:\")\n",
    "    print(langs)\n",
    "except Exception as e:\n",
    "    print(f\"Could not get languages: {e}\")\n",
    "    print(\"Common languages: eng, fra, deu, spa, ita, por, chi_sim, jpn, kor, ara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample images with different languages\n",
    "# Note: For non-English languages, you need to install the language packs\n",
    "# Ubuntu: sudo apt-get install tesseract-ocr-<lang>\n",
    "\n",
    "# English text\n",
    "eng_text = pytesseract.image_to_string(pil_image, lang='eng')\n",
    "print(\"English (eng):\")\n",
    "print(eng_text)\n",
    "\n",
    "# Multiple languages (if you have text mixed with different languages)\n",
    "# Use '+' to combine languages\n",
    "try:\n",
    "    multi_lang_text = pytesseract.image_to_string(pil_image, lang='eng+fra')\n",
    "    print(\"\\nEnglish + French (eng+fra):\")\n",
    "    print(multi_lang_text)\n",
    "except Exception as e:\n",
    "    print(f\"Multi-language not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512aeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image with special characters/symbols\n",
    "special_img = Image.new('RGB', (500, 150), color='white')\n",
    "draw = ImageDraw.Draw(special_img)\n",
    "\n",
    "try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 28)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "# Text with numbers and symbols\n",
    "special_text = \"Price: $99.99 (50% OFF!)\"\n",
    "draw.text((30, 50), special_text, fill='black', font=font)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(special_img)\n",
    "plt.title(\"Text with Numbers and Symbols\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Extract with different configs\n",
    "extracted = pytesseract.image_to_string(special_img, lang='eng', config='--psm 7')\n",
    "print(f\"Extracted: {extracted.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93540f",
   "metadata": {},
   "source": [
    "## 8. Preprocess Images for Better OCR Results\n",
    "\n",
    "Apply image preprocessing techniques like grayscale conversion, thresholding, and noise removal to improve OCR accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noisy/degraded image for demonstration\n",
    "noisy_img = Image.new('RGB', (600, 150), color=(240, 240, 240))\n",
    "draw = ImageDraw.Draw(noisy_img)\n",
    "\n",
    "try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 28)\n",
    "except OSError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "# Add text with low contrast\n",
    "draw.text((50, 50), \"Noisy Text Sample 123\", fill=(100, 100, 100), font=font)\n",
    "\n",
    "# Add noise\n",
    "np_noisy = np.array(noisy_img)\n",
    "noise = np.random.randint(-30, 30, np_noisy.shape, dtype=np.int16)\n",
    "np_noisy = np.clip(np_noisy.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "noisy_img = Image.fromarray(np_noisy)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(noisy_img)\n",
    "plt.title(\"Original Noisy Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"OCR on noisy image:\")\n",
    "print(pytesseract.image_to_string(noisy_img, config='--psm 7').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image, method='threshold'):\n",
    "    \"\"\"\n",
    "    Preprocess image for better OCR results.\n",
    "    \n",
    "    Methods:\n",
    "    - 'grayscale': Convert to grayscale\n",
    "    - 'threshold': Binary thresholding\n",
    "    - 'adaptive': Adaptive thresholding\n",
    "    - 'denoise': Noise removal\n",
    "    - 'full': All preprocessing steps\n",
    "    \"\"\"\n",
    "    # Convert PIL to OpenCV format\n",
    "    img = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = img.copy()\n",
    "    \n",
    "    if method == 'grayscale':\n",
    "        return gray\n",
    "    \n",
    "    elif method == 'threshold':\n",
    "        # Otsu's thresholding\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return thresh\n",
    "    \n",
    "    elif method == 'adaptive':\n",
    "        # Adaptive thresholding\n",
    "        thresh = cv2.adaptiveThreshold(\n",
    "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        return thresh\n",
    "    \n",
    "    elif method == 'denoise':\n",
    "        # Denoise\n",
    "        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "        return denoised\n",
    "    \n",
    "    elif method == 'full':\n",
    "        # Full preprocessing pipeline\n",
    "        # 1. Denoise\n",
    "        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "        # 2. Threshold\n",
    "        _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return thresh\n",
    "    \n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different preprocessing methods\n",
    "methods = ['grayscale', 'threshold', 'adaptive', 'denoise', 'full']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(noisy_img)\n",
    "axes[0].set_title(f\"Original\\nOCR: '{pytesseract.image_to_string(noisy_img, config='--psm 7').strip()}'\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Preprocessed versions\n",
    "for i, method in enumerate(methods, 1):\n",
    "    processed = preprocess_image(noisy_img, method)\n",
    "    ocr_result = pytesseract.image_to_string(processed, config='--psm 7').strip()\n",
    "    \n",
    "    axes[i].imshow(processed, cmap='gray')\n",
    "    axes[i].set_title(f\"{method.capitalize()}\\nOCR: '{ocr_result}'\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional preprocessing: Deskewing and scaling\n",
    "def deskew(image):\n",
    "    \"\"\"Deskew an image using moments.\"\"\"\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    if len(coords) == 0:\n",
    "        return image\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, \n",
    "                              borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def scale_image(image, scale_factor=2):\n",
    "    \"\"\"Scale image for better OCR (Tesseract works best with DPI >= 300).\"\"\"\n",
    "    width = int(image.shape[1] * scale_factor)\n",
    "    height = int(image.shape[0] * scale_factor)\n",
    "    return cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Demo scaling\n",
    "processed = preprocess_image(noisy_img, 'full')\n",
    "scaled = scale_image(processed, 2)\n",
    "\n",
    "print(f\"Original size: {processed.shape}\")\n",
    "print(f\"Scaled size: {scaled.shape}\")\n",
    "\n",
    "ocr_original = pytesseract.image_to_string(processed, config='--psm 7').strip()\n",
    "ocr_scaled = pytesseract.image_to_string(scaled, config='--psm 7').strip()\n",
    "\n",
    "print(f\"\\nOCR (original): '{ocr_original}'\")\n",
    "print(f\"OCR (scaled 2x): '{ocr_scaled}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
